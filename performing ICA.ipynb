{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d7e92c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T12:08:48.445000-07:00",
     "start_time": "2025-09-17T19:08:43.457Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.11.7\n",
      "Commit f2b3dbda30a (2025-09-08 12:10 UTC)\n",
      "Build Info:\n",
      "  Official https://julialang.org/ release\n",
      "Platform Info:\n",
      "  OS: Linux (x86_64-linux-gnu)\n",
      "  CPU: 24 × Intel(R) Core(TM) i9-9920X CPU @ 3.50GHz\n",
      "  WORD_SIZE: 64\n",
      "  LLVM: libLLVM-16.0.6 (ORCJIT, skylake-avx512)\n",
      "Threads: 24 default, 0 interactive, 12 GC (on 24 virtual cores)\n",
      "Environment:\n",
      "  JULIA_NUM_THREADS = auto\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Documents/Codes/Julia/ICAmm`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mProject\u001b[22m\u001b[39m ICAmm v0.1.0\n",
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/Documents/Codes/Julia/ICAmm/Project.toml`\n",
      "  \u001b[90m[13e28ba4] \u001b[39mAppleAccelerate v0.4.1\n",
      "  \u001b[90m[6e4b80f9] \u001b[39mBenchmarkTools v1.6.0\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[052768ef] \u001b[39mCUDA v5.8.2\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[992eb4ea] \u001b[39mCondaPkg v0.2.31\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[a93c6f00] \u001b[39mDataFrames v1.7.0\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[31c24e10] \u001b[39mDistributions v0.25.120\n",
      "  \u001b[90m[46192b85] \u001b[39mGPUArraysCore v0.2.0\n",
      "  \u001b[90m[7a12625a] \u001b[39mLinearMaps v3.11.4\n",
      "  \u001b[90m[bdcacae8] \u001b[39mLoopVectorization v0.12.172\n",
      "  \u001b[90m[23992714] \u001b[39mMAT v0.10.7\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[dde4c033] \u001b[39mMetal v1.6.2\n",
      "  \u001b[90m[6f286f6a] \u001b[39mMultivariateStats v0.10.3\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[91a5bcdd] \u001b[39mPlots v1.40.20\n",
      "  \u001b[90m[21216c6a] \u001b[39mPreferences v1.5.0\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[08abe8d2] \u001b[39mPrettyTables v2.4.0\n",
      "  \u001b[90m[438e738f] \u001b[39mPyCall v1.96.4\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[6099a3de] \u001b[39mPythonCall v0.9.27\n",
      "  \u001b[90m[1fd47b50] \u001b[39mQuadGK v2.11.2\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[f2b01f46] \u001b[39mRoots v2.2.8\n",
      "  \u001b[90m[10745b16] \u001b[39mStatistics v1.11.1\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[6aa20fa7] \u001b[39mTensorOperations v5.2.0\n",
      "  \u001b[90m[a759f4b9] \u001b[39mTimerOutputs v0.5.29\n",
      "  \u001b[90m[bc48ee85] \u001b[39mTullio v0.3.8\n",
      "  \u001b[90m[8149f6b0] \u001b[39mWAV v1.2.0\n",
      "  \u001b[90m[37e2e46d] \u001b[39mLinearAlgebra v1.11.0\n",
      "  \u001b[90m[9a3f8284] \u001b[39mRandom v1.11.0\n",
      "\u001b[36m\u001b[1mInfo\u001b[22m\u001b[39m Packages marked with \u001b[32m⌃\u001b[39m and \u001b[33m⌅\u001b[39m have new versions available. Those with \u001b[32m⌃\u001b[39m may be upgradable, but those with \u001b[33m⌅\u001b[39m are restricted by compatibility constraints from upgrading. To see why use `status --outdated`\n"
     ]
    }
   ],
   "source": [
    "versioninfo()\n",
    "\n",
    "using Pkg\n",
    "\n",
    "Pkg.activate(pwd())\n",
    "Pkg.instantiate()\n",
    "Pkg.status()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e8a1dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T12:09:00.726000-07:00",
     "start_time": "2025-09-17T19:08:48.082Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size :(20 64246)\n"
     ]
    }
   ],
   "source": [
    "using LinearAlgebra, Random, Plots, BenchmarkTools, MultivariateStats\n",
    "using WAV, MAT, Statistics\n",
    "\n",
    "# using ICAmm\n",
    "using Distributions, LoopVectorization\n",
    "using TimerOutputs\n",
    "using Base.Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1bf96",
   "metadata": {},
   "source": [
    "# MM Algorithm for ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bad19fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "icastruct"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "using LinearAlgebra, Random, Metal, CUDA, Distributions, LoopVectorization, AppleAccelerate\n",
    "\n",
    "function create_array(data::Union{AbstractMatrix{T}, AbstractVector{T}}, use_gpu::Bool=false, use_metal::Bool=false) where T <: AbstractFloat\n",
    "    if use_gpu\n",
    "        @assert isdefined(Main, :CUDA) \"CUDA.jl is not loaded. Please install and load CUDA.jl.\"\n",
    "        return isa(data, CuArray) ? data : CuArray(data)\n",
    "    elseif use_metal\n",
    "        @assert isdefined(Main, :Metal) \"Metal.jl is not loaded. Please install and load Metal.jl.\"\n",
    "        return isa(data, MtlArray) ? data : MtlArray(data)\n",
    "    else\n",
    "        return isa(data, Array) ? data : Array(data)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Define the icastruct\n",
    "mutable struct icastruct{T <: AbstractFloat, A <: AbstractArray{T}}\n",
    "    X :: A\n",
    "    W :: A\n",
    "    Y :: A\n",
    "    M_storage1 :: A\n",
    "    M_storage2 :: A\n",
    "    M_storage3 :: A\n",
    "    M_storage4 :: A\n",
    "    M_storage5 :: A\n",
    "    E_storage :: AbstractVector{T}\n",
    "    G :: A\n",
    "    G_old :: A\n",
    "    psiY :: A\n",
    "    psidY :: A\n",
    "    direction :: A\n",
    "    I_storage :: A\n",
    "    h :: A\n",
    "end\n",
    "\n",
    "function icastruct(X::AbstractMatrix{T}, use_gpu::Bool=false, use_metal::Bool=false) where T <: AbstractFloat\n",
    "    # Convert input matrix to the appropriate device (CPU/GPU)\n",
    "    X_device = create_array(X, use_gpu, use_metal)\n",
    "    m, n = size(X_device)\n",
    "    @assert n ≥ m \"Expect more samples (columns) than dimensions (rows) in X\"\n",
    "    \n",
    "    # Initialize other fields with consistent types\n",
    "    W = create_array(Matrix{T}(I, m, m), use_gpu, use_metal)\n",
    "    M_storage1 = create_array(zeros(T, m, m), use_gpu, use_metal)\n",
    "    M_storage2 = create_array(zeros(T, m, m), use_gpu, use_metal)\n",
    "    M_storage3 = create_array(zeros(T, m, m), use_gpu, use_metal)\n",
    "    M_storage4 = create_array(zeros(T, m, n), use_gpu, use_metal)\n",
    "    M_storage5 = create_array(zeros(T, m, n), use_gpu, use_metal)\n",
    "    I_storage = create_array(Matrix{T}(I, m, m), use_gpu, use_metal)\n",
    "    E_storage = create_array(zeros(T, m), use_gpu, use_metal)  # Reshape as 2D array\n",
    "    \n",
    "    Y = similar(X_device)\n",
    "    Y .= 0\n",
    "\n",
    "    return icastruct(X_device, \n",
    "        W, \n",
    "        Y,\n",
    "        M_storage1, \n",
    "        M_storage2, \n",
    "        M_storage3, \n",
    "        M_storage4, \n",
    "        M_storage5, \n",
    "        E_storage,\n",
    "        similar(W),\n",
    "        similar(W),\n",
    "        similar(X),\n",
    "        similar(X),\n",
    "        similar(W),\n",
    "        I_storage,\n",
    "        similar(W))\n",
    "end\n",
    "\n",
    "# eltype(::icastruct{T, A, B}) where {T, A, B} = T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e53aa67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ica_ken (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function threaded_tanh!(Z::AbstractMatrix, Y::AbstractMatrix)\n",
    "    @threads for j in 1:size(Y, 2)\n",
    "        @turbo for i in 1:size(Y, 1)\n",
    "            Z[i, j] = tanh(Y[i, j]/2)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function whitening(X)\n",
    "    cov_X = cov(X', corrected=false)\n",
    "    eigenvalues, eigenvectors = eigen(cov_X)\n",
    "    D_inv_sqrt = Diagonal(1 ./ sqrt.(eigenvalues))\n",
    "    W = eigenvectors * D_inv_sqrt * eigenvectors'\n",
    "    X_whitened = W * X\n",
    "    return X_whitened, W\n",
    "end\n",
    "\n",
    "function score_cpu!(icas::icastruct{T, M}) where {T <: AbstractFloat, M <: AbstractMatrix{T}}\n",
    "#     @. ica.M_storage3 .= ica.Y ./ T(2)\n",
    "    threaded_tanh!(icas.psiY, icas.Y)\n",
    "end\n",
    "\n",
    "function score_gpu!(icas::icastruct{T, M}) where {T<:AbstractFloat, M<:AbstractMatrix{T}}\n",
    "    half = eltype(icas.Y)(0.5f0)\n",
    "    @. icas.psiY = tanh(half * icas.Y)\n",
    "end\n",
    "\n",
    "function _logabsdet_any(A::AbstractMatrix{T}) where {T<:AbstractFloat}\n",
    "    F = lu(A)\n",
    "    U = UpperTriangular(F.U)\n",
    "    s = zero(T)\n",
    "    @inbounds @tturbo for i in 1:size(U,1)\n",
    "        s += log(abs(U[i,i]))\n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "function _log2cosh_sums_threaded!(buf::AbstractMatrix{T}, Y::AbstractMatrix{T}) where {T<:AbstractFloat}\n",
    "    nt = Threads.nthreads()\n",
    "    s_abs_t  = zeros(T, nt)\n",
    "    s_tail_t = zeros(T, nt)\n",
    "    @inbounds Threads.@threads for j in axes(Y,2)\n",
    "        tid = Threads.threadid()\n",
    "        sA = zero(T); sT = zero(T)\n",
    "        @simd for i in axes(Y,1)\n",
    "            a = abs(Y[i,j])\n",
    "            sA += a\n",
    "            t = log1p(exp(-a))  # stable\n",
    "            sT += t\n",
    "            buf[i,j] = t\n",
    "        end\n",
    "        s_abs_t[tid]  += sA\n",
    "        s_tail_t[tid] += sT\n",
    "    end\n",
    "    return sum(s_abs_t), sum(s_tail_t)\n",
    "end\n",
    "\n",
    "function loss(icas::icastruct{T,M}, Y::M, W::M) where {T<:AbstractFloat,M<:AbstractMatrix{T}}\n",
    "    n = size(Y, 2)\n",
    "    log_det = log(abs(det(W)))\n",
    "    s_abs, s_tail = _log2cosh_sums_threaded!(icas.M_storage4, Y)\n",
    "    logcosh_sum = s_abs + 2*s_tail\n",
    "    return -log_det + logcosh_sum / n\n",
    "end\n",
    "\n",
    "function gradient(icas::icastruct{T, M}) where {T<:AbstractFloat, M<:AbstractMatrix{T}}\n",
    "    m, n = size(icas.Y)\n",
    "    copyto!(icas.M_storage1, icas.I_storage)\n",
    "    α = one(T) / T(n)\n",
    "    mul!(icas.M_storage1, icas.psiY, transpose(icas.Y), -α, one(T))\n",
    "    return icas.M_storage1\n",
    "end\n",
    "\n",
    "# Generate data for testing\n",
    "function generate_data(m::Int, n::Int; use_gpu::Bool=false, use_metal::Bool=false)\n",
    "    S1 = create_array(randn(m, n), use_gpu, use_metal)\n",
    "    B = create_array(randn(m, m), use_gpu, use_metal)\n",
    "    return B * S1\n",
    "end\n",
    "\n",
    "using TimerOutputs\n",
    "\n",
    "function ica_ken(X::AbstractMatrix{T};\n",
    "                 maxiter::Int = 1000, \n",
    "                 tol = 1e-6, \n",
    "                 verbose::Bool = false, \n",
    "                 W_warmStart::Union{AbstractMatrix{T}, Nothing} = nothing,\n",
    "                 nesterov::Bool = true) where {T <: AbstractFloat}\n",
    "\n",
    "    use_metal = isa(X, MtlMatrix)\n",
    "    use_gpu = isa(X, CuArray)\n",
    "\n",
    "    if use_metal || use_gpu\n",
    "#         println(\"using gpu\")\n",
    "        tol = Float32(tol)\n",
    "        ONE = 1f0\n",
    "        TWO = 2f0\n",
    "        HALF = 0.5f0\n",
    "        sTWO = sqrt(2f0)\n",
    "        sHALF = sqrt(1/(2f0))\n",
    "    else\n",
    "        tol = T(tol) \n",
    "        ONE = T(1)\n",
    "        TWO = T(2)\n",
    "        HALF = T(0.5)\n",
    "        sTWO = sqrt(T(2))\n",
    "        sHALF = sqrt(1/T(2))\n",
    "#         println(\"using cpu\")\n",
    "    end\n",
    "\n",
    "    icas = icastruct(X, use_gpu, use_metal)\n",
    "    m, n = size(icas.X)\n",
    "    log_liks = create_array(zeros(T, 0), use_gpu, use_metal)\n",
    "\n",
    "    gradient_norm = ONE\n",
    "    current_loss = nothing\n",
    "    final_loss = nothing\n",
    "    \n",
    "    D = similar(icas.W)\n",
    "    mul!(D, X, transpose(X), HALF/T(n), 0)\n",
    "    D = sHALF.*D\n",
    "    c = -sHALF/T(n)\n",
    "    XT2n = c.*transpose(icas.X)   # XT2n/\n",
    "        \n",
    "    icas.G_old .= similar(icas.W)\n",
    "    icas.G .= similar(icas.W)\n",
    "    W_old = similar(icas.W)\n",
    "    copyto!(W_old, icas.W)\n",
    "    W_prev = similar(icas.W)\n",
    "    \n",
    "    t_k = T(1)\n",
    "    t_new = T(1)\n",
    "    if W_warmStart != nothing\n",
    "        copyto!(icas.W, W_warmStart)\n",
    "    end\n",
    "    \n",
    "    mul!(icas.Y, icas.W, icas.X)\n",
    "    A_cpu = Matrix(icas.M_storage1)\n",
    "    \n",
    "    niters = maxiter\n",
    "    for iter in 1:maxiter\n",
    "        \n",
    "        if use_metal || use_gpu\n",
    "            score_gpu!(icas)\n",
    "        else\n",
    "            score_cpu!(icas)\n",
    "        end\n",
    "        \n",
    "        if mod(iter, Int(5)) == 0\n",
    "            icas.G .= gradient(icas)\n",
    "            gradient_norm = norm(icas.G, Inf)\n",
    "            if gradient_norm < tol\n",
    "                niters = iter\n",
    "                break\n",
    "            end\n",
    "            \n",
    "            if verbose\n",
    "                current_loss = loss(icas, icas.Y, icas.W)\n",
    "                println(\"iteration \", iter, \", gradient norm: \", gradient_norm,\n",
    "                    \", loglikelihood: \", current_loss)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        ## Calculating C^T = 1/(4n)*A*X^T = 0.5*XXT/(2n)-psiY*XT2n = 0.5*D-psiY*XT2n\n",
    "        ## Calculating icas.M_storage2 = D^(-1/2)*C^T = sqrt(T(2)) C^T\n",
    "        mul!(icas.M_storage2, icas.W, D)\n",
    "        mul!(icas.M_storage2, icas.psiY, XT2n, ONE, ONE)  # C^T * D^(-1/2)\n",
    "        \n",
    "        icas.M_storage1 = transpose(icas.M_storage2)  ## M = D^(-1/2) C\n",
    "        \n",
    "        ## using SVD\n",
    "        copyto!(A_cpu, icas.M_storage1)\n",
    "        S = svd(A_cpu)\n",
    "        copyto!(icas.M_storage1, create_array(S.U, use_gpu, use_metal))\n",
    "        copyto!(icas.M_storage2, create_array(S.V, use_gpu, use_metal))\n",
    "        copyto!(icas.E_storage, sqrt.(S.S .^ TWO .+ ONE) .+ S.S)\n",
    "        # V[Sigma + sqrt(Sigma^2 + 1)]U^T D^{-1/2}\n",
    "        Dia = Diagonal(icas.E_storage) \n",
    "        mul!(icas.W, icas.M_storage2, Dia)\n",
    "        mul!(icas.M_storage3, icas.W, transpose(icas.M_storage1))\n",
    "#         mul!(icas.W, icas.M_storage3, icas.D_inv_sqrt)\n",
    "        icas.W .= sTWO.*icas.M_storage3\n",
    "\n",
    "        if nesterov\n",
    "            t_new = (ONE + sqrt(ONE + TWO*TWO * t_k^TWO)) / TWO\n",
    "        else\n",
    "            t_new = t_k\n",
    "        end\n",
    "        \n",
    "        θ = (t_k - ONE) / t_new\n",
    "        icas.M_storage1 .= icas.W - W_old             # ΔW = W - W_old\n",
    "        copyto!(W_old, icas.W)                        # W_old = W\n",
    "         @. icas.W .= icas.W + θ * icas.M_storage1    # fused update\n",
    "\n",
    "        mul!(icas.Y, icas.W, icas.X)      # matrix multiply on GPU\n",
    "        t_k = t_new\n",
    "    end\n",
    "#     final_loss = loss(icas, icas.Y, icas.W)\n",
    "    return icas.W, niters, final_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e3f0437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size :(20 64246)\n"
     ]
    }
   ],
   "source": [
    "# filename = \"./faster-ica/examples/eeg.mat\"\n",
    "filename = \"./faster-ica/examples/fmri.mat\"\n",
    "\n",
    "file = matopen(filename)\n",
    "X = read(file, \"X\")\n",
    "close(file)\n",
    "X_mean = mean(X, dims=2)\n",
    "X = X .- X_mean\n",
    "X_whitened, _ = whitening(X)\n",
    "n_features, n_samples = size(X_whitened)\n",
    "println(\"size :\",\"(\", n_features, \" \", n_samples, \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436d2ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d61b9258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 119 samples with 1 evaluation per sample.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m36.682 ms\u001b[22m\u001b[39m … \u001b[35m73.955 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 16.24%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m38.694 ms              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m42.090 ms\u001b[22m\u001b[39m ± \u001b[32m 7.283 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m3.95% ±  6.21%\n",
       "\n",
       "  \u001b[39m▄\u001b[39m█\u001b[39m \u001b[39m \u001b[34m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m█\u001b[39m█\u001b[39m▆\u001b[39m▅\u001b[34m▆\u001b[39m\u001b[39m▆\u001b[39m▃\u001b[39m▆\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[32m▄\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m \u001b[39m▃\n",
       "  36.7 ms\u001b[90m         Histogram: frequency by time\u001b[39m        63.5 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m13.00 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m70465\u001b[39m."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fmri\n",
    "X_whitened_cpu = Float32.(X_whitened)\n",
    "X_whitened_GPU = cu(X_whitened_cpu)\n",
    "tol = 1e-4\n",
    "@benchmark W_GPU, nters_GPU, loss_GPU = ica_ken($X_whitened_GPU,\n",
    "    maxiter=500,\n",
    "    nesterov=true,\n",
    "    tol=tol,\n",
    "    verbose=false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf755e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b91dd557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 22 samples with 1 evaluation per sample.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m217.108 ms\u001b[22m\u001b[39m … \u001b[35m274.581 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 12.78%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m230.920 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.88%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m234.020 ms\u001b[22m\u001b[39m ± \u001b[32m 12.989 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m1.43% ±  2.68%\n",
       "\n",
       "  \u001b[39m▃\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▃\u001b[39m \u001b[39m \u001b[39m \u001b[39m█\u001b[34m \u001b[39m\u001b[39m \u001b[39m▃\u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m▁\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▁\u001b[39m█\u001b[34m▁\u001b[39m\u001b[39m▇\u001b[39m█\u001b[39m▁\u001b[32m▇\u001b[39m\u001b[39m▇\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m▇\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m▁\u001b[39m▇\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m \u001b[39m▁\n",
       "  217 ms\u001b[90m           Histogram: frequency by time\u001b[39m          275 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m32.01 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m12997\u001b[39m."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tol = 1e-4\n",
    "@benchmark W_cpu, nters_cpu, loss_cpu = ica_ken($X_whitened_cpu,\n",
    "    maxiter=500,\n",
    "    nesterov=true,\n",
    "    tol=tol,\n",
    "    verbose=false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab4a1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2805fbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 13 samples with 1 evaluation per sample.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m361.198 ms\u001b[22m\u001b[39m … \u001b[35m481.603 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m1.00% … 19.16%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m379.437 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.62%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m385.854 ms\u001b[22m\u001b[39m ± \u001b[32m 30.718 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m2.28% ±  5.19%\n",
       "\n",
       "  \u001b[39m▁\u001b[39m \u001b[39m▁\u001b[39m█\u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▁\u001b[34m \u001b[39m\u001b[39m█\u001b[39m▁\u001b[39m \u001b[32m \u001b[39m\u001b[39m▁\u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \n",
       "  \u001b[39m█\u001b[39m▁\u001b[39m█\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m█\u001b[34m▁\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m▁\u001b[32m▁\u001b[39m\u001b[39m█\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m \u001b[39m▁\n",
       "  361 ms\u001b[90m           Histogram: frequency by time\u001b[39m          482 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m62.73 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m13549\u001b[39m."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tol = 1e-4\n",
    "@benchmark W, nters, _ = ica_ken($X_whitened,\n",
    "    maxiter=500,\n",
    "    nesterov=true,\n",
    "    tol=tol,\n",
    "    verbose=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe9d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b60a2ef7",
   "metadata": {},
   "source": [
    "## Running in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63b6a65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_python (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function _log2cosh_sums_threaded!(buf::AbstractMatrix{T}, Y::AbstractMatrix{T}) where {T<:AbstractFloat}\n",
    "    nt = Threads.nthreads()\n",
    "    s_abs_t  = zeros(T, nt)\n",
    "    s_tail_t = zeros(T, nt)\n",
    "    @inbounds Threads.@threads for j in axes(Y,2)\n",
    "        tid = Threads.threadid()\n",
    "        sA = zero(T); sT = zero(T)\n",
    "        @simd for i in axes(Y,1)\n",
    "            a = abs(Y[i,j])\n",
    "            sA += a\n",
    "            t = log1p(exp(-a))  # stable\n",
    "            sT += t\n",
    "            buf[i,j] = t\n",
    "        end\n",
    "        s_abs_t[tid]  += sA\n",
    "        s_tail_t[tid] += sT\n",
    "    end\n",
    "    return sum(s_abs_t), sum(s_tail_t)\n",
    "end\n",
    "\n",
    "\n",
    "function loss_python(Y, W)\n",
    "    n = size(Y, 2)\n",
    "    M_storage4 = similar(Y)\n",
    "    log_det = _logabsdet_any(W)   # your existing routine\n",
    "    s_abs, s_tail = _log2cosh_sums_threaded!(M_storage4, Y)\n",
    "    logcosh_sum = s_abs + 2*s_tail\n",
    "    return -log_det + logcosh_sum / n\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77ce2790",
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyCall\n",
    "\n",
    "sys = pyimport(\"sys\"); \n",
    "pushfirst!(PyVector(sys.\"path\"),\n",
    "           \"/home/xunjianli/Documents/Codes/Julia/ICAmm/faster-ica\")\n",
    "pyimport(\"ml_ica\")\n",
    "\n",
    "py\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ml_ica.tools import whitening, callback\n",
    "from ml_ica.algorithms import (\n",
    "    picard, simple_quasi_newton_ica, truncated_ica, trust_region_ica\n",
    ")\n",
    "\n",
    "def _cb_get(cb, key):\n",
    "    try:\n",
    "        return cb[key]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def run_ica_collect(mat_path, tol=1e-4, max_iter=250, double=True, out_png=None):\n",
    "    X = loadmat(mat_path)['X']\n",
    "    X = X - X.mean(axis=1, keepdims=True)\n",
    "    X, _ = whitening(X)\n",
    "\n",
    "    algos  = [truncated_ica, trust_region_ica, simple_quasi_newton_ica, picard]\n",
    "    names  = ['Truncated Newton ICA', 'Trust region ICA', 'Simple quasi-Newton ICA', 'Picard']\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    if out_png is not None:\n",
    "        plt.figure()\n",
    "\n",
    "    for algo, name in zip(algos, names):\n",
    "        if name == 'Trust region ICA':\n",
    "            continue\n",
    "\n",
    "        cb = callback(['timing', 'gradient_norm', 'loss', 'objective'])\n",
    "\n",
    "        if double:\n",
    "            X_copy = X.copy()\n",
    "        else:\n",
    "            X_copy = X.astype(np.float32, copy=True)\n",
    "\n",
    "        Y, W = algo(X_copy, verbose=1, callback=cb, tol=tol, max_iter=max_iter)\n",
    "\n",
    "        times = _cb_get(cb, 'timing') or []\n",
    "        grads = _cb_get(cb, 'gradient_norm') or []\n",
    "        loss  = _cb_get(cb, 'loss')\n",
    "        if loss is None:\n",
    "            loss = _cb_get(cb, 'objective')\n",
    "\n",
    "        times = np.asarray(times, dtype=float) if len(times)>0 else None\n",
    "        grads = np.asarray(grads, dtype=float) if len(grads)>0 else None\n",
    "        loss  = np.asarray(loss,  dtype=float) if (loss is not None and len(loss)>0) else None\n",
    "\n",
    "        results[name] = {\n",
    "            'Ys': Y,\n",
    "            'Ws': W,\n",
    "            'times': times,\n",
    "            'grads': grads,\n",
    "            'loss' : loss,\n",
    "            'iters': int(len(times)) if times is not None else 0\n",
    "        }\n",
    "\n",
    "        if out_png is not None and (times is not None) and (grads is not None):\n",
    "            plt.semilogy(times, grads, label=name)\n",
    "\n",
    "    if out_png is not None:\n",
    "        plt.legend(); plt.xlabel('Time (sec)'); plt.ylabel('Gradient norm')\n",
    "        plt.tight_layout(); plt.savefig(out_png, dpi=150); plt.close()\n",
    "\n",
    "    return results\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca8bcc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, gradient norm = 0.6129\r",
      "iteration 1, gradient norm = 0.4592\r",
      "iteration 2, gradient norm = 0.3048\r",
      "iteration 3, gradient norm = 0.1751\r",
      "iteration 4, gradient norm = 0.08854\r",
      "iteration 5, gradient norm = 0.06195\r",
      "iteration 6, gradient norm = 0.07277\r",
      "iteration 7, gradient norm = 0.1068\r",
      "iteration 8, gradient norm = 0.07751\r",
      "iteration 9, gradient norm = 0.07905\r",
      "iteration 10, gradient norm = 0.07482\r",
      "iteration 11, gradient norm = 0.04282\r",
      "iteration 12, gradient norm = 0.1503\r",
      "iteration 13, gradient norm = 0.2527\r",
      "iteration 14, gradient norm = 0.04532\r",
      "iteration 15, gradient norm = 0.08844\r",
      "iteration 16, gradient norm = 0.008313\r",
      "iteration 17, gradient norm = 0.007702\r",
      "iteration 18, gradient norm = 0.001353\r",
      "iteration 19, gradient norm = 0.001786\r",
      "iteration 20, gradient norm = 0.0001164\r",
      "iteration 0, gradient norm = 0.6129\r",
      "iteration 1, gradient norm = 0.946\r",
      "iteration 2, gradient norm = 1.108\r",
      "iteration 3, gradient norm = 0.9155\r",
      "iteration 4, gradient norm = 0.9149\r",
      "iteration 5, gradient norm = 0.5428\r",
      "iteration 6, gradient norm = 0.4896\r",
      "iteration 7, gradient norm = 0.2994\r",
      "iteration 8, gradient norm = 0.3059\r",
      "iteration 9, gradient norm = 0.4111\r",
      "iteration 10, gradient norm = 0.1579\r",
      "iteration 11, gradient norm = 0.04831\r",
      "iteration 12, gradient norm = 0.024\r",
      "iteration 13, gradient norm = 0.0161\r",
      "iteration 14, gradient norm = 0.01541\r",
      "iteration 15, gradient norm = 0.01256\r",
      "iteration 16, gradient norm = 0.01215\r",
      "iteration 17, gradient norm = 0.01019\r",
      "iteration 18, gradient norm = 0.01037\r",
      "iteration 19, gradient norm = 0.008526\r",
      "iteration 20, gradient norm = 0.009241\r",
      "iteration 21, gradient norm = 0.007169\r",
      "iteration 22, gradient norm = 0.008455\r",
      "iteration 23, gradient norm = 0.005885\r",
      "iteration 24, gradient norm = 0.007875\r",
      "iteration 25, gradient norm = 0.004661\r",
      "iteration 26, gradient norm = 0.0074\r",
      "iteration 27, gradient norm = 0.005062\r",
      "iteration 28, gradient norm = 0.007765\r",
      "iteration 29, gradient norm = 0.006005\r",
      "iteration 30, gradient norm = 0.01045\r",
      "iteration 31, gradient norm = 0.008106\r",
      "iteration 32, gradient norm = 0.01418\r",
      "iteration 33, gradient norm = 0.01053\r",
      "iteration 34, gradient norm = 0.01394\r",
      "iteration 35, gradient norm = 0.009154\r",
      "iteration 36, gradient norm = 0.009529\r",
      "iteration 37, gradient norm = 0.00559\r",
      "iteration 38, gradient norm = 0.00528\r",
      "iteration 39, gradient norm = 0.002921\r",
      "iteration 40, gradient norm = 0.002707\r",
      "iteration 41, gradient norm = 0.001482\r",
      "iteration 42, gradient norm = 0.001378\r",
      "iteration 43, gradient norm = 0.0007652\r",
      "iteration 44, gradient norm = 0.000715\r",
      "iteration 45, gradient norm = 0.0004058\r",
      "iteration 46, gradient norm = 0.0003803\r",
      "iteration 47, gradient norm = 0.0002205\r",
      "iteration 48, gradient norm = 0.0002258\r",
      "iteration 49, gradient norm = 0.0001296\r",
      "iteration 50, gradient norm = 0.0001357\r",
      "iteration 0, gradient norm = 0.6129\r",
      "iteration 1, gradient norm = 0.946\r",
      "iteration 2, gradient norm = 0.7596\r",
      "iteration 3, gradient norm = 0.9762\r",
      "iteration 4, gradient norm = 0.7721\r",
      "iteration 5, gradient norm = 0.8759\r",
      "iteration 6, gradient norm = 0.7379\r",
      "iteration 7, gradient norm = 0.7448\r",
      "iteration 8, gradient norm = 0.6557\r",
      "iteration 9, gradient norm = 0.6247\r",
      "iteration 10, gradient norm = 0.5999\r",
      "iteration 11, gradient norm = 0.6125\r",
      "iteration 12, gradient norm = 0.2489\r",
      "iteration 13, gradient norm = 0.185\r",
      "iteration 14, gradient norm = 0.4564\r",
      "iteration 15, gradient norm = 0.1714\r",
      "iteration 16, gradient norm = 0.1452\r",
      "iteration 17, gradient norm = 0.1482\r",
      "iteration 18, gradient norm = 0.1232\r",
      "iteration 19, gradient norm = 0.06547\r",
      "iteration 20, gradient norm = 0.1093\r",
      "iteration 21, gradient norm = 0.04327\r",
      "iteration 22, gradient norm = 0.01044\r",
      "iteration 23, gradient norm = 0.0177\r",
      "iteration 24, gradient norm = 0.008054\r",
      "iteration 25, gradient norm = 0.001514\r",
      "iteration 26, gradient norm = 0.00191\r",
      "iteration 27, gradient norm = 0.001624\r",
      "iteration 28, gradient norm = 0.0003429\r",
      "iteration 29, gradient norm = 0.000543\r",
      "iteration 30, gradient norm = 0.0005486\r",
      "iteration 31, gradient norm = 0.0001727\r",
      "iteration 32, gradient norm = 0.0001818\r",
      "iteration 33, gradient norm = 0.0003346\r",
      "iteration 34, gradient norm = 0.0002093\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Any, Any} with 3 entries:\n",
       "  \"Simple quasi-Newton ICA\" => Dict{Any, Any}(\"grads\"=>[0.612867, 0.945988, 1.1…\n",
       "  \"Picard\"                  => Dict{Any, Any}(\"grads\"=>[0.612867, 0.945988, 0.7…\n",
       "  \"Truncated Newton ICA\"    => Dict{Any, Any}(\"grads\"=>[0.612867, 0.459176, 0.3…"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ml = pyimport(\"ml_ica\")\n",
    "root = \"/home/xunjianli/Documents/Codes/Julia/ICAmm/faster-ica\"\n",
    "# matfile = joinpath(root, \"examples\", \"eeg.mat\")\n",
    "matfile = joinpath(root, \"examples\", \"fmri.mat\")\n",
    "res_py = py\"run_ica_collect\"(matfile, 1e-4, 250, true, \"/tmp/ica_grad.png\")  ## choosing double precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e77e0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, PrettyTables, Printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3896346e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e93e07c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────────────────┬────────────┬───────────┬───────────┬───────────┐\n",
      "│\u001b[1m                  Method \u001b[0m│\u001b[1m  FinalGrad \u001b[0m│\u001b[1m FinalTime \u001b[0m│\u001b[1m Finaliter \u001b[0m│\u001b[1m FinalLoss \u001b[0m│\n",
      "│\u001b[90m                  String \u001b[0m│\u001b[90m     String \u001b[0m│\u001b[90m    String \u001b[0m│\u001b[90m    String \u001b[0m│\u001b[90m    String \u001b[0m│\n",
      "├─────────────────────────┼────────────┼───────────┼───────────┼───────────┤\n",
      "│    Truncated Newton ICA │ 4.4537e-05 │    1.1783 │        22 │ 26.538976 │\n",
      "│ Simple quasi-Newton ICA │ 1.3573e-04 │    0.5973 │        51 │ 26.538977 │\n",
      "│                  Picard │ 2.0932e-04 │    0.5253 │        35 │ 26.538976 │\n",
      "└─────────────────────────┴────────────┴───────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "names  = [\"Truncated Newton ICA\", \"Simple quasi-Newton ICA\", \"Picard\"]\n",
    "Ws  = [res_py[n][\"Ws\"] for n in names]\n",
    "Ys  = [res_py[n][\"Ys\"] for n in names]\n",
    "grads  = [res_py[n][\"grads\"][end] for n in names]\n",
    "times  = [res_py[n][\"times\"][end] for n in names]\n",
    "iters  = [res_py[n][\"iters\"][end] for n in names]\n",
    "\n",
    "function loss_python(Y, W)\n",
    "    n = size(Y, 2)\n",
    "    M_storage4 = similar(Y)\n",
    "    log_det = _logabsdet_any(W)   # your existing routine\n",
    "    s_abs, s_tail = _log2cosh_sums_threaded!(M_storage4, Y)\n",
    "    logcosh_sum = s_abs + 2*s_tail\n",
    "    return -log_det + logcosh_sum / n\n",
    "end\n",
    "\n",
    "losses = [loss_python(Ys[i], Ws[i]) for i in eachindex(names)]\n",
    "\n",
    "df_show = DataFrame(\n",
    "    Method    = names,\n",
    "    FinalGrad = [@sprintf(\"%.4e\", g) for g in grads],\n",
    "    FinalTime = [@sprintf(\"%.4f\", t) for t in times],\n",
    "    Finaliter = [@sprintf(\"%d\", it) for it in iters],\n",
    "    FinalLoss = [@sprintf(\"%.6f\", ℓ) for ℓ in losses],\n",
    ")\n",
    "\n",
    "pretty_table(df_show)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f1db4c",
   "metadata": {},
   "source": [
    "## Singele precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2caaae49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, gradient norm = 0.6129\r",
      "iteration 1, gradient norm = 0.4592\r",
      "iteration 2, gradient norm = 0.3048\r",
      "iteration 3, gradient norm = 0.1751\r",
      "iteration 4, gradient norm = 0.08854\r",
      "iteration 5, gradient norm = 0.06195\r",
      "iteration 6, gradient norm = 0.07277\r",
      "iteration 7, gradient norm = 0.1068\r",
      "iteration 8, gradient norm = 0.07751\r",
      "iteration 9, gradient norm = 0.07905\r",
      "iteration 10, gradient norm = 0.07482\r",
      "iteration 11, gradient norm = 0.04282\r",
      "iteration 12, gradient norm = 0.1503\r",
      "iteration 13, gradient norm = 0.2527\r",
      "iteration 14, gradient norm = 0.04532\r",
      "iteration 15, gradient norm = 0.08844\r",
      "iteration 16, gradient norm = 0.008313\r",
      "iteration 17, gradient norm = 0.007702\r",
      "iteration 18, gradient norm = 0.001353\r",
      "iteration 19, gradient norm = 0.001786\r",
      "iteration 20, gradient norm = 0.0001164\r",
      "iteration 0, gradient norm = 0.6129\r",
      "iteration 1, gradient norm = 0.946\r",
      "iteration 2, gradient norm = 1.108\r",
      "iteration 3, gradient norm = 0.9155\r",
      "iteration 4, gradient norm = 0.9149\r",
      "iteration 5, gradient norm = 0.5427\r",
      "iteration 6, gradient norm = 0.4896\r",
      "iteration 7, gradient norm = 0.2994\r",
      "iteration 8, gradient norm = 0.3059\r",
      "iteration 9, gradient norm = 0.411\r",
      "iteration 10, gradient norm = 0.1579\r",
      "iteration 11, gradient norm = 0.04826\r",
      "iteration 12, gradient norm = 0.02398\r",
      "iteration 13, gradient norm = 0.01609\r",
      "iteration 14, gradient norm = 0.01541\r",
      "iteration 15, gradient norm = 0.01255\r",
      "iteration 16, gradient norm = 0.01215\r",
      "iteration 17, gradient norm = 0.01019\r",
      "iteration 18, gradient norm = 0.01037\r",
      "iteration 19, gradient norm = 0.008521\r",
      "iteration 20, gradient norm = 0.009241\r",
      "iteration 21, gradient norm = 0.007163\r",
      "iteration 22, gradient norm = 0.008454\r",
      "iteration 23, gradient norm = 0.005877\r",
      "iteration 24, gradient norm = 0.007875\r",
      "iteration 25, gradient norm = 0.004667\r",
      "iteration 26, gradient norm = 0.007399\r",
      "iteration 27, gradient norm = 0.005071\r",
      "iteration 28, gradient norm = 0.007788\r",
      "iteration 29, gradient norm = 0.006022\r",
      "iteration 30, gradient norm = 0.01049\r",
      "iteration 31, gradient norm = 0.008138\r",
      "iteration 32, gradient norm = 0.01421\r",
      "iteration 33, gradient norm = 0.01054\r",
      "iteration 34, gradient norm = 0.01391\r",
      "iteration 35, gradient norm = 0.009121\r",
      "iteration 36, gradient norm = 0.009478\r",
      "iteration 37, gradient norm = 0.005554\r",
      "iteration 38, gradient norm = 0.005245\r",
      "iteration 39, gradient norm = 0.0029\r",
      "iteration 40, gradient norm = 0.002688\r",
      "iteration 41, gradient norm = 0.001471\r",
      "iteration 42, gradient norm = 0.001368\r",
      "iteration 43, gradient norm = 0.00076\r",
      "iteration 44, gradient norm = 0.0007101\r",
      "iteration 45, gradient norm = 0.0004032\r",
      "iteration 46, gradient norm = 0.0003779\r",
      "iteration 47, gradient norm = 0.0002191\r",
      "iteration 48, gradient norm = 0.0002246\r",
      "iteration 49, gradient norm = 0.0001289\r",
      "iteration 50, gradient norm = 0.000135\r",
      "iteration 0, gradient norm = 0.6129\r",
      "iteration 1, gradient norm = 0.946\r",
      "iteration 2, gradient norm = 0.7596\r",
      "iteration 3, gradient norm = 0.9762\r",
      "iteration 4, gradient norm = 0.7721\r",
      "iteration 5, gradient norm = 0.8759\r",
      "iteration 6, gradient norm = 0.738\r",
      "iteration 7, gradient norm = 0.7448\r",
      "iteration 8, gradient norm = 0.6557\r",
      "iteration 9, gradient norm = 0.6247\r",
      "iteration 10, gradient norm = 0.6\r",
      "iteration 11, gradient norm = 0.6127\r",
      "iteration 12, gradient norm = 0.2489\r",
      "iteration 13, gradient norm = 0.1847\r",
      "iteration 14, gradient norm = 0.4571\r",
      "iteration 15, gradient norm = 0.1715\r",
      "iteration 16, gradient norm = 0.1458\r",
      "iteration 17, gradient norm = 0.1484\r",
      "iteration 18, gradient norm = 0.1238\r",
      "iteration 19, gradient norm = 0.06593\r",
      "iteration 20, gradient norm = 0.1085\r",
      "iteration 21, gradient norm = 0.04256\r",
      "iteration 22, gradient norm = 0.0103\r",
      "iteration 23, gradient norm = 0.01771\r",
      "iteration 24, gradient norm = 0.008028\r",
      "iteration 25, gradient norm = 0.001513\r",
      "iteration 26, gradient norm = 0.001916\r",
      "iteration 27, gradient norm = 0.001613\r",
      "iteration 28, gradient norm = 0.0003414\r",
      "iteration 29, gradient norm = 0.0005466\r",
      "iteration 30, gradient norm = 0.0005466\r",
      "iteration 31, gradient norm = 0.0001729\r",
      "iteration 32, gradient norm = 0.0001841\r",
      "iteration 33, gradient norm = 0.0003345\r",
      "iteration 34, gradient norm = 0.0002076\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Any, Any} with 3 entries:\n",
       "  \"Simple quasi-Newton ICA\" => Dict{Any, Any}(\"grads\"=>[0.612867, 0.945988, 1.1…\n",
       "  \"Picard\"                  => Dict{Any, Any}(\"grads\"=>[0.612867, 0.945988, 0.7…\n",
       "  \"Truncated Newton ICA\"    => Dict{Any, Any}(\"grads\"=>[0.612867, 0.459176, 0.3…"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = pyimport(\"ml_ica\")\n",
    "root = \"/home/xunjianli/Documents/Codes/Julia/ICAmm/faster-ica\"\n",
    "# matfile = joinpath(root, \"examples\", \"eeg.mat\")\n",
    "matfile = joinpath(root, \"examples\", \"fmri.mat\")\n",
    "res_py = py\"run_ica_collect\"(matfile, 1e-4, 250, false, \"/tmp/ica_grad.png\")  ## choose single precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f0e9bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────────────────┬────────────┬───────────┬───────────┬───────────┐\n",
      "│\u001b[1m                  Method \u001b[0m│\u001b[1m  FinalGrad \u001b[0m│\u001b[1m FinalTime \u001b[0m│\u001b[1m Finaliter \u001b[0m│\u001b[1m FinalLoss \u001b[0m│\n",
      "│\u001b[90m                  String \u001b[0m│\u001b[90m     String \u001b[0m│\u001b[90m    String \u001b[0m│\u001b[90m    String \u001b[0m│\u001b[90m    String \u001b[0m│\n",
      "├─────────────────────────┼────────────┼───────────┼───────────┼───────────┤\n",
      "│    Truncated Newton ICA │ 4.4538e-05 │    1.1904 │        22 │ 26.538976 │\n",
      "│ Simple quasi-Newton ICA │ 1.3500e-04 │    0.7477 │        51 │ 26.538977 │\n",
      "│                  Picard │ 2.0763e-04 │    0.6108 │        35 │ 26.538976 │\n",
      "└─────────────────────────┴────────────┴───────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "names  = [\"Truncated Newton ICA\", \"Simple quasi-Newton ICA\", \"Picard\"]\n",
    "Ws  = [res_py[n][\"Ws\"] for n in names]\n",
    "Ys  = [res_py[n][\"Ys\"] for n in names]\n",
    "grads  = [res_py[n][\"grads\"][end] for n in names]\n",
    "times  = [res_py[n][\"times\"][end] for n in names]\n",
    "iters  = [res_py[n][\"iters\"][end] for n in names]\n",
    "\n",
    "function loss_python(Y, W)\n",
    "    n = size(Y, 2)\n",
    "    M_storage4 = similar(Y)\n",
    "    log_det = _logabsdet_any(W)   # your existing routine\n",
    "    s_abs, s_tail = _log2cosh_sums_threaded!(M_storage4, Y)\n",
    "    logcosh_sum = s_abs + 2*s_tail\n",
    "    return -log_det + logcosh_sum / n\n",
    "end\n",
    "\n",
    "losses = [loss_python(Ys[i], Ws[i]) for i in eachindex(names)]\n",
    "\n",
    "df_show = DataFrame(\n",
    "    Method    = names,\n",
    "    FinalGrad = [@sprintf(\"%.4e\", g) for g in grads],\n",
    "    FinalTime = [@sprintf(\"%.4f\", t) for t in times],\n",
    "    Finaliter = [@sprintf(\"%d\", it) for it in iters],\n",
    "    FinalLoss = [@sprintf(\"%.6f\", ℓ) for ℓ in losses],\n",
    ")\n",
    "\n",
    "pretty_table(df_show)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0daecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.7",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
